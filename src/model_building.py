import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, roc_curve
from xgboost import XGBClassifier
import joblib # For saving models
import os

# --- 1. Custom Evaluation Metrics ---

def calculate_gini(y_true, y_pred_proba):
    """Calculates the Gini coefficient from AUC score."""
    auc = roc_auc_score(y_true, y_pred_proba)
    # Gini = 2 * AUC - 1
    return 2 * auc - 1

def calculate_ks(y_true, y_pred_proba):
    """Calculates the Kolmogorov-Smirnov (KS) statistic."""
    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
    # KS is the maximum difference between TPR and FPR
    return np.max(tpr - fpr)

# --- 2. Data Loading and Preparation ---

# Assuming the WoE-transformed data is correctly generated by previous scripts
DATA_PATH = 'data/rfms_woe_transformed_data.csv'
TARGET_COLUMN = 'Default_Proxy'

try:
    df = pd.read_csv(DATA_PATH)
except FileNotFoundError:
    print(f"Error: Data file not found at {DATA_PATH}. Please run feature_engineering.py and woe_iv_transformation.py first.")
    exit()

# Separate features (X) and target (y)
X = df.drop(columns=[TARGET_COLUMN, 'Customer_ID'], errors='ignore')
y = df[TARGET_COLUMN]

# Split data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# --- 3. Primary Model: WoE Logistic Regression (Scorecard Base) ---

print("\n--- Training Primary Model: WoE Logistic Regression ---")

# Define parameter grid for regularization tuning
# 'C' is the inverse of regularization strength; smaller C means stronger regularization
param_grid_lr = {'C': [0.01, 0.1, 1.0, 10.0]}

# Setup Grid Search with Cross-Validation (CV=5)
grid_search_lr = GridSearchCV(
    LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42),
    param_grid_lr,
    scoring='roc_auc',
    cv=5,
    verbose=1,
    n_jobs=-1
)
grid_search_lr.fit(X_train, y_train)

best_lr_model = grid_search_lr.best_estimator_
lr_best_score = grid_search_lr.best_score_
print(f"Best CV AUC for LogReg: {lr_best_score:.4f} (C={best_lr_model.C})")

# Evaluate Logistic Regression on Test Set
y_pred_proba_lr = best_lr_model.predict_proba(X_test)[:, 1]
lr_test_auc = roc_auc_score(y_test, y_pred_proba_lr)
lr_test_gini = calculate_gini(y_test, y_pred_proba_lr)
lr_test_ks = calculate_ks(y_test, y_pred_proba_lr)

print("\n--- Logistic Regression Test Metrics ---")
print(f"AUC: {lr_test_auc:.4f}")
print(f"GINI: {lr_test_gini:.4f}")
print(f"KS: {lr_test_ks:.4f}")

# --- 4. Benchmarking Model: XGBoost ---

print("\n--- Training Benchmarking Model: XGBoost ---")

# Define parameter grid for XGBoost tuning (Simplified for efficiency)
param_grid_xgb = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5],
    'learning_rate': [0.05, 0.1]
}

# Setup Grid Search with Cross-Validation (CV=5)
grid_search_xgb = GridSearchCV(
    XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
    param_grid_xgb,
    scoring='roc_auc',
    cv=5,
    verbose=1,
    n_jobs=-1
)
grid_search_xgb.fit(X_train, y_train)

best_xgb_model = grid_search_xgb.best_estimator_
xgb_best_score = grid_search_xgb.best_score_
print(f"Best CV AUC for XGBoost: {xgb_best_score:.4f}")

# Evaluate XGBoost on Test Set
y_pred_proba_xgb = best_xgb_model.predict_proba(X_test)[:, 1]
xgb_test_auc = roc_auc_score(y_test, y_pred_proba_xgb)
xgb_test_gini = calculate_gini(y_test, y_pred_proba_xgb)
xgb_test_ks = calculate_ks(y_test, y_pred_proba_xgb)

print("\n--- XGBoost Test Metrics (Benchmark) ---")
print(f"AUC: {xgb_test_auc:.4f}")
print(f"GINI: {xgb_test_gini:.4f}")
print(f"KS: {xgb_test_ks:.4f}")

# --- 5. Save Final Model Artifacts and Coefficients ---

MODEL_DIR = 'models'
os.makedirs(MODEL_DIR, exist_ok=True)

# 5.1 Save the final scorecard model (LogReg) and the best benchmark model
joblib.dump(best_lr_model, os.path.join(MODEL_DIR, 'final_scorecard_logreg.pkl'))
joblib.dump(best_xgb_model, os.path.join(MODEL_DIR, 'benchmark_xgb.pkl'))

# 5.2 Extract and save Logistic Regression coefficients for FastAPI/Scorecard implementation
# This is crucial for transparent deployment and avoids loading the entire scikit-learn object.
coefficients_df = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': best_lr_model.coef_[0]
})
# Add the intercept (a.k.a. base odds)
intercept_df = pd.DataFrame({
    'Feature': ['Intercept'],
    'Coefficient': [best_lr_model.intercept_[0]]
})
final_coefficients_df = pd.concat([coefficients_df, intercept_df], ignore_index=True)

COEFF_PATH = os.path.join(MODEL_DIR, 'logreg_coefficients.csv')
final_coefficients_df.to_csv(COEFF_PATH, index=False)

print(f"\nSuccessfully trained, tuned, and evaluated both models.")
print(f"Final LogReg scorecard model saved to: {os.path.join(MODEL_DIR, 'final_scorecard_logreg.pkl')}")
print(f"Benchmark XGBoost model saved to: {os.path.join(MODEL_DIR, 'benchmark_xgb.pkl')}")
print(f"LogReg Scorecard Coefficients saved to: {COEFF_PATH}")